# Modelo LLaMA com Quantização e Parâmetros de Geração Personalizados
Este modelo utiliza o LLaMA-30b para carregar modelos de linguagem e gerar respostas a partir de instruções fornecidas pelo usuário.
A quantização é aplicada para reduzir o uso de memória, tornando possível executar modelos maiores em GPUs com menos VRAM disponível.

![tela](https://user-images.githubusercontent.com/101840230/230735310-62ac8398-ff4e-43d0-96d8-b9abce73fdf8.png)

(readme feito pela IA)
